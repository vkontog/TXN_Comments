{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "qualified-english",
   "metadata": {},
   "source": [
    "<b>Import necessery libraries</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "casual-renewal",
   "metadata": {
    "gather": {
     "logged": 1642687197211
    }
   },
   "outputs": [],
   "source": [
    "#!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1b16725",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xlwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "advised-contact",
   "metadata": {
    "gather": {
     "logged": 1642687213435
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from azureml.core import Experiment\n",
    "from azureml.core import Workspace, Dataset\n",
    "from azureml.data import DataType\n",
    "from spacy.cli.download import download as spacy_download\n",
    "import os \n",
    "from os.path import join as osjoin\n",
    "import xlwt\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9163eae5",
   "metadata": {
    "gather": {
     "logged": 1642687213579
    }
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-franchise",
   "metadata": {},
   "source": [
    "<b>Select the default workspace & datastore</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "lesbian-teddy",
   "metadata": {
    "gather": {
     "logged": 1642687218160
    }
   },
   "outputs": [],
   "source": [
    "# azureml-core of version 1.0.72 or higher is required\n",
    "# azureml-dataprep[pandas] of version 1.1.34 or higher is required\n",
    "from azureml.core import Workspace, Dataset\n",
    "\n",
    "subscription_id = '6ed9d167-b2e6-41b8-9500-35e6df64d9dc'\n",
    "resource_group = 'MLRG'\n",
    "workspace_name = 'erbbimlws'\n",
    "\n",
    "workspace = Workspace(subscription_id, resource_group, workspace_name)\n",
    "\n",
    "\n",
    "datastore = workspace.get_default_datastore()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-timing",
   "metadata": {},
   "source": [
    "<b>Loading the Greek language tools</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "clear-segment",
   "metadata": {
    "gather": {
     "logged": 1642687224041
    },
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('el_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "spacy_download('el_core_news_sm')\n",
    "nlp =spacy.load('el_core_news_sm', disable=['tagger', 'parser', 'ner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coated-alarm",
   "metadata": {},
   "source": [
    "<b>Parameter definitions</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "documentary-cleaners",
   "metadata": {
    "gather": {
     "logged": 1642687224183
    }
   },
   "outputs": [],
   "source": [
    "#minimum number of tokens in the texts\n",
    "minCount = 500\n",
    "#ngrams parameters\n",
    "mindf,minngram,maxngram = 30,2,3\n",
    "#keep empty tokens\n",
    "deleteEmptyTokens = True\n",
    "#dataset name to be analyzed\n",
    "datasetName = 'Txns_NLP_202101'\n",
    "#export filename\n",
    "fileName = 'Txns_NLP_202101_exp'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-basics",
   "metadata": {},
   "source": [
    "<b>Regular expressions definitions</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "greenhouse-chicago",
   "metadata": {
    "gather": {
     "logged": 1642687224476
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "p1 = re.compile('δεν απαντ.{1,3}\\s{0,1}',re.IGNORECASE)\n",
    "p2 = re.compile('\\sδα\\s',re.IGNORECASE)\n",
    "p3 = re.compile('δε.{0,1}\\s.{0,3}\\s{0,1}βρ.{1,2}κ.\\s{0,1}',re.IGNORECASE)\n",
    "p4 = re.compile('[^\\d]?\\d{10}')\n",
    "p5 = re.compile('[^\\d]?\\d{18}|[^\\d]\\d{20}')\n",
    "p6 = re.compile('δε[ ν]{0,1} (επιθυμ[α-ω]{2,4}?|[ήη]θ[εέ]λ[α-ω]{1,3}?|θελ[α-ω]{1,4}|.{0,20}ενδιαφ[εέ]ρ[α-ω]{2,4})',re.IGNORECASE)\n",
    "p7 = re.compile('δε[ ν]{0,1} (μπορ[α-ω]{2,5}|.εχει)',re.IGNORECASE)\n",
    "p8 = re.compile('(δεν|μη).*διαθεσιμ[οη]ς{0,1}?',re.IGNORECASE)\n",
    "p9 = re.compile('(δεν|μη)+.*εφικτη?',re.IGNORECASE)\n",
    "p10 = re.compile('δε[ ν]{0,1}.{0,20}θετικ[οόήη]ς{0,1}',re.IGNORECASE)\n",
    "\n",
    "#pinakides\n",
    "#p11 = re.compile('\\s([a-zA-Zα-ωΑ-Ω]{3}\\s*?[0-9]{3,4})\\s',re.IGNORECASE)\n",
    "p11 = re.compile('(^|\\s)[a-zA-Zα-ωΑ-Ω]{3}[0-9]{3,4}(\\s|$)',re.IGNORECASE)\n",
    "\n",
    "#enoikia\n",
    "p12=re.compile('εν(.{1,2}κ.{1,3})',re.IGNORECASE)\n",
    "p13=re.compile('en(.{1,2}k.{1,3})',re.IGNORECASE)\n",
    "\n",
    "p14 = re.compile('(γιατρος|giatros|φυσιοθεραπ|ορθοπαιδικος|ΩΡΛ|ΩΝΑΣΕΙΟ|ψυχολογος|ψυχοθεραπεια|ψυχιατρος|LOGO -ERGOTHERAPEIA|VIODIAGNOSI|KLINIKI|ΚΛΙΝΙΚΗ)\\S*' ,re.IGNORECASE)\n",
    "p15 = re.compile('(EX/SH|Εξοφληση|ΕΞΟΦΛΗΣΗ|eksoflisi|eksoflhsh|ΕΞΩΦΛΗΣΗ|Εξόφλησ|EXOFLISI|EXOFLHSH|EJOFLHSH|EΞΟΦΛ|εξοφ|εξοφλ|εξοφλη|εξοφλης|εξόφληση|εξφ|εξωφ|εξωφλ)\\S*' ,re.IGNORECASE)\n",
    "p16 = re.compile('(ASFALIA|asfaleia|ασφάλ|asfalistra|ΑΣΦΑΛΕΙΑ|ΑΣΦΑΛΕΙΑΣ|ασφαλειες|ασφάλειες|ασφαλιστηριο|ασφαλιστρα|ασφαλιστρων)\\S*' ,re.IGNORECASE)\n",
    "p17 = re.compile('(ΤΙΜΟΛΟΓΙΑ|ΤΙΜΟΛΟΓΙΟ|ΤΙΜΟΛΟΓΙΩΝ|ΤΙΜΟΛΟΓΊΟΥ|ΠΡΟΦΟΡΜΑ|PROFORMA|INVOICE|TIMOLOGIWN|TIMOLOGIA|TIMOLOGIOU|timologio|timologion|timologioy)\\S*' ,re.IGNORECASE)\n",
    "p18 = re.compile('(ΝΕΡΟΥ|ΝΕΡΟ|ευδαπ|nero|nerou|eydap|PLIROMI NEROU|ΠΛΗΡΩΜΗ ΝΕΡΟΥ)\\S*' ,re.IGNORECASE)\n",
    "p38 = re.compile('(^|\\s)(ΔΕΗ|dei|DEH|ρευμα|reuma|ILEKTRIKOU|ILEKTRIKO|ηλεκτρικο)\\S*' ,re.IGNORECASE)\n",
    "p19 = re.compile('(ΜΙΣΘΟΔΟΣΙΑ|MISTHODOSIA|DORO CHRISTOUGENNON|DORO PASHA|DORO PASXA|apodoches|apodochon|μισθ|μισθου|salary|MISTH|Αδεια|ΑΠΟΔΟΧΕΣ|αποδοχων|μηνιατικο|μισθοδοσία|ΕΞΟΦΛ ΜΙΣΘ|ΜΙΣΘΟΣ|MISTHOS|misthou|ΜΙΣΘΟΔΩΣΙΑ|ΜΙΣΘ/ΣΙΑ|ΔΩΡΟ ΧΡΙΣΟΥΓΕΝΩΝ|ΔΩΡΟ ΧΡΙΣΤ|DWRO XRISTOYGENNWN|ΔΩΡΟ ΠΑΣΧΑ|ΟΔΟΙΠΟΡΙΚΑ|PAYROLL|MISTHODOSIAS|MISTHODOSIES|ODOIPORIKA|EPIDOMA ADEIAS|ΕΠΙΔΟΜΑ ΑΔΕΙΑΣ)\\S*' ,re.IGNORECASE)\n",
    "p41 = re.compile('(ΕΝΦΙΑ|enfia)\\S*' ,re.IGNORECASE)\n",
    "p20 = re.compile('(ΤΑΠ ΑΚΙΝΗΤΟΥ|ΚΤΗΜΑΤΟΛΟΓΙΟ)\\S*' ,re.IGNORECASE)\n",
    "p21 = re.compile('(^|\\s)(foros|φορος|φορος εισοδηματος|FOROI|eforia|εφορια)\\S*' ,re.IGNORECASE)\n",
    "p42 = re.compile('(^|\\s)(ΕΦΚΑ|efka)(\\s|$)' ,re.IGNORECASE)\n",
    "p43 = re.compile('(^|\\s)(fpa|ΦΠΑ)(\\s|$)*' ,re.IGNORECASE)\n",
    "p44 = re.compile('(^|\\s)(ΙΚΑ|IKA)(\\s|$)',re.IGNORECASE)\n",
    "p45 = re.compile('(^|\\s)(TEBE|TEVE|ΤΕΒΕ|oaee|ΟΑΕΕ)(\\s|$)' ,re.IGNORECASE)\n",
    "p22 = re.compile('(KOINOXΡHΣTA|ΚΟΙΝ/ΣΤΑ|κοινοχρηστων|κοινοχρ|Συντηρηση Ανελκυστηρα|Κοινόχροιστα|KOINOXRISTA|KOINOCHRISTA|PLIROMI KOINOCHRISTON|koinoxrhsta)\\S*' ,re.IGNORECASE)\n",
    "p23 = re.compile('(να ζησετε|δωρο γαμου|na zisete|gamou|gamos|γαμος)\\S*' ,re.IGNORECASE)\n",
    "p78 = re.compile('(^|\\s)(ζησετε|zisete)(\\s|$)' ,re.IGNORECASE)\n",
    "p24 = re.compile('(να σας ζησει|na sas zisei|βαφτιση|vaftisi|baftisi|σας ζησει|sas zisei)\\S*' ,re.IGNORECASE)\n",
    "p79 = re.compile('(^|\\s)(ζησει|zisei)(\\s|$)' ,re.IGNORECASE)\n",
    "p25 = re.compile('(stegastiko|δοση σπίτι|Δάνειο σπίτι|δόση στεγαστικού δανείου|στεγαστικου|στεγαστικο|ΣΤΕΓΑΣΤΙΚΟ  ΔΑΝΕΙΟ|Πληρωμη στεγαστικου|mortgage installment|mortgage|STEGASTIKOU|EPISKEVASTIKOU|επισκευαστικο|episkeuastiko)\\S*' ,re.IGNORECASE)\n",
    "p26 = re.compile('(μτφ|metafora|μεταφορα|mtf|METAF)\\S*' ,re.IGNORECASE)\n",
    "p27 = re.compile('(ORDER|παραγγελίας|ΠΑΡΑΓΓΕΛΙΑ|αρ.παρ.|PARAGGELIAS|PARAGGELIA|αρ.παρ|orders|PARANGELIA)\\S*' ,re.IGNORECASE)\n",
    "p28 = re.compile('(xreos|ΔΑΝΕΙΚΑ|ΕΝΑΝΤΙ ΛΟΓΑΡΙΑΣΜΟΥ|εξοδα|ΕΞΟΔ.|exoda|expenses|οφειλη|χρεος|ENANTI LOGARIASMOU|DANEIKA|OFEILES)\\S*' ,re.IGNORECASE)\n",
    "p29 = re.compile('(κτθ|καταθεση|katathesi|ktth)\\S*' ,re.IGNORECASE)\n",
    "p30 = re.compile('(ΠΡΟΚΑΤΑ|ΠΡΟΚΑΤΑΒΟΛΉ|ΠΡΟΚΑΤΑΒ|prokatavoli|prokataboli|prokatav|PROKATABOLH|προκατ|προκαταβολες)\\S*' ,re.IGNORECASE)\n",
    "p31 = re.compile('(DIATROFI|ΔΙΑΤΡΟΦΗ|ΔΙΑΤΡΟΦΙ|DIA/FI|ΔΙΑ/ΦΗ)(\\s|$)' ,re.IGNORECASE)\n",
    "p32 = re.compile('(δοση αυτοκινητου|δανειο αυτοκινητου|δανειο αυτοκινητο|daneio aftokinitou|Πληρωμη Αυτοκινητο|Αυτοκινητο πληρωμη|αυτοκινητο δοση|daneiou autokinitou|daneio autokinitou|dosi autokonitou|daneiou autokinito)\\S*' ,re.IGNORECASE)\n",
    "#p76 = re.compile('^(αυτοκινητο|autokinito)$' ,re.IGNORECASE)\n",
    "p76 = re.compile('(^|\\s)(αυτοκινητο|autokinito|αυτοκινητου|aftokinitou)\\S*' ,re.IGNORECASE)\n",
    "p77 = re.compile('(agora autokinitou|αγορα αυτοκινητου|αγορα αυτοκινητο|agora aftokinitou)\\S*' ,re.IGNORECASE)\n",
    "p33 = re.compile('(ΠΛΗΡΩΜΗ ΔΑΝΕΙΟΥ|daneio|DANEIOU|δανειο|δανειου|daneioy|δαν|δανε|dosi daniou|εξοικονομω|Πληρωμη δοση|Πληρωμη δοσης|δοση δανειου|ΔΟΣΗ ΔΑΝΕΙΟΥ|DOSΗ ΔΑΝΕΙΟΥ|ΔΌΣΗΔΑΝΕΊΟΥ|ΠΛΥΡΟΜΗ.ΔΑΝΕΙΟΥ|PLIROMI DANEIOU|DOSI DANEIOU|PLHPOMH DANEIOY|PLIROMI DOSIS DANEIOU|danio|DANEIO )\\S*' ,re.IGNORECASE)\n",
    "p34 = re.compile('(Πληρωμή καρτας|ΠΛΗΡΩΜΉ ΠΙΣΤΩΤΙΚΗΣ|card pay|card payment)\\S*' ,re.IGNORECASE)\n",
    "p47 = re.compile('(dwrea|donation|δωρεα|dorea)\\S*' ,re.IGNORECASE)\n",
    "p48 = re.compile('(^|\\s)(dosi|dosis|doseis|dosh|δοση|δοσης|δοσεις)\\S*' ,re.IGNORECASE)\n",
    "p49 = re.compile('(ALPHA BANK|OPTIMA BANK|PEIRAIOS|ETHNIKI|PIRAEUS|ALPHA)\\S*' ,re.IGNORECASE)\n",
    "p70 = re.compile('(^|\\s)(ετε|ete)(\\s|$)',re.IGNORECASE)\n",
    "p50 = re.compile('(ELLINOAMERIKANIKI|EKPAIDEFTIRIA|EKPAIDEFTIKA|ELLINOAMERIKANIKO|ΕΛΛΗΝΟΑΜΕΡΙΚΑΝΙΚΗ|ΕΛΛΗΝΟΑΜΕΡΙΚΑΝΙΚΟ|ΕΚΠΑΙΔΕΥΤΗΡΙΑ)\\S*' ,re.IGNORECASE)\n",
    "p51 = re.compile('(ΟΑΕΔ|OAED)\\S*' ,re.IGNORECASE)\n",
    "p52 = re.compile('(ΤΕΛΗ ΚΥΚΛΟΦΟΡΙΑΣ|TELI KYKLOFORIAS)\\S*' ,re.IGNORECASE)\n",
    "p53 = re.compile('(^|\\s)(ΕΠΙΤΑΓΗ|ΕΠΙΤΑΓΕΣ|ΕΠΙΤΑΓΗΣ|EPITAGHS|EPITAGES|EPITAGH|EPITAGI)\\S*' ,re.IGNORECASE)\n",
    "p54 = re.compile('(MERISMATOS|MERISMA|MERISMATA|MERISMATON|ΜΕΡΙΣΜΑ|ΜΕΡΙΣΜΑΤΑ)\\S*' ,re.IGNORECASE)\n",
    "p55 = re.compile('(COLLEGE|ΚΟΛΛΕΓΙΟ|kolegio)\\S*' ,re.IGNORECASE)\n",
    "p56 = re.compile('(^|\\s)(VODAFON|Cosmote|WIND|FORTHNET|OTE)(\\s|$)*' ,re.IGNORECASE)\n",
    "p57 = re.compile('(DIDAKTRA|ΔΙΔΑΚΤΡΑ|διδακτρων|didaktron)\\S*' ,re.IGNORECASE)\n",
    "p59 = re.compile('(ΝΟΜΟΣ 3869|ΝΟΜΟΣ3869|Ν.3869)\\S*' ,re.IGNORECASE)\n",
    "p60 = re.compile('(Ασφαλεια αυτο/του|ασφαλεια αυτοκινητου|ασφαλειων αυτ/των|ασφαλεια αυτοκινητο|asfaleia aftokinitou|Ασφαλιστηριο αυτοκινητου|Ασφαλιστρα αυτοκινητου|ασφαλιση αυτοκινητου)\\S*' ,re.IGNORECASE)\n",
    "p61 = re.compile('(^|\\s)(Ασφαλεια σπιτιου|asfaleia spitiou|ΑΣΦΆΛΕΙΑ ΣΠΙΤΙΟΎ|asfalia spitiou|πυρασφαλεια|ΠΥΡΟΣ ΣΕΙΣΜΟΥ|ΠΥΡΟΣ)(\\s|$)' ,re.IGNORECASE)\n",
    "p62 = re.compile('(enoik|noiki|rent|νοικι)\\S*' ,re.IGNORECASE)\n",
    "p63 = re.compile('(payment|payments|plhpomh|pliromes|pliromi|pliromis|πληρωμες|πληρωμη)\\S*' ,re.IGNORECASE)\n",
    "p64 = re.compile('(συμβ|συμβολαιο|συμβολαιου|symv)\\S*' ,re.IGNORECASE)\n",
    "p65 = re.compile('(επιδομα|επιδοματος|epidoma|epidomatos)\\S*' ,re.IGNORECASE)\n",
    "p66 = re.compile('(κρατηση|κρατησης|kratisi|kratisis|booking)\\S*' ,re.IGNORECASE)\n",
    "p67 = re.compile('(diakopes|διακοπες|κρατηση δωματιου|kratisi dwmatiou|kratisi domatiou|κρατηση ξενοδοχειου|κρατηση δικλινου|κρατηση τρικλινου|κρατηση καταλυματος|κρατηση δωματιων|δικλινο|τρικλινο|diklino|triklino|δικλινου|τρικλινου|diklinou|triklinou)\\S*' ,re.IGNORECASE)\n",
    "p68 = re.compile('(petrelaio|πετρελαιο|πετρελαιου|petreleo|petrelaiou)\\S*' ,re.IGNORECASE)\n",
    "p69 = re.compile('(kausima|kafsima|καυσιμα)\\S*' ,re.IGNORECASE)\n",
    "p71 = re.compile('(αγορα|αγορας|αγορες|agora|agoras)\\S*' ,re.IGNORECASE)\n",
    "p75 = re.compile('(ANGLIKA|agglika|gallika|αγγλικα|γαλλικα|ΦΡΟΝΤΙΣΤΗΡΙΟ|frontistirio|frontistiriou|frontisthrio|φροντιστηριου)\\S*' ,re.IGNORECASE)\n",
    " \n",
    "\n",
    "#timologia\n",
    "p35 = re.compile('\\s?(tim|τιμ|τιμολ|timol)\\s?\\.?\\d{1,30}\\s?',re.IGNORECASE)\n",
    "p80 = re.compile('(^|\\s)(tim|τιμ|τιμολ|timol)\\s?\\.?(\\s|$)',re.IGNORECASE)\n",
    "p72 = re.compile('\\s?(τδα|tda)\\s?\\.?\\d{1,30}\\s?',re.IGNORECASE)\n",
    "p46 = re.compile('\\s?(inv)\\s?\\.?\\d{1,30}\\s?',re.IGNORECASE)\n",
    "#μηνες για εξαιρεση από πινακιδες\n",
    "p73 = re.compile('\\s?(ιαν|φεβ|μαρ|απρ|μαι|ιου|αυγ|σεπ|οκτ|νοε|δεκ)\\s?\\.?\\d{1,30}\\s?',re.IGNORECASE)\n",
    "p74 = re.compile('\\s?(jan|feb|mar|apr|may|jun|jul|aug|sep|okt|nov|dec)\\s?\\.?\\d{1,30}\\s?',re.IGNORECASE)\n",
    "#afm\n",
    "p36 = re.compile('\\s?(αφμ|afm)\\s?\\.?\\d{1,9}\\s?',re.IGNORECASE)\n",
    "p37 = re.compile('(ΑΦΜ|AFM)\\S*' ,re.IGNORECASE)\n",
    "#transfers\n",
    "p39 = re.compile('\\s?(POO)\\s?\\.?\\s?',re.IGNORECASE)\n",
    "p40 = re.compile('\\s?(POI)\\s?\\.?\\s?',re.IGNORECASE)\n",
    "#characters\n",
    "p58 = re.compile('\\s?[\\W|_]{1}\\s?',re.IGNORECASE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-bobby",
   "metadata": {},
   "source": [
    "<b>Functions definitions</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "permanent-tolerance",
   "metadata": {
    "gather": {
     "logged": 1642687224653
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def loadStopWords(ws):\n",
    "    #A dataset containing the Greek stop words has been created\n",
    "    #the function loads this dataset as a dataframe\n",
    "    dataset = Dataset.get_by_name(ws, name='stopWords_gr')\n",
    "    sw = set(dataset.to_pandas_dataframe())\n",
    "    return sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "documentary-nelson",
   "metadata": {
    "gather": {
     "logged": 1642687224803
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def replaceTerm(text):\n",
    "    #This function uses the above defined regular expressions to replace text\n",
    "    #The order of the rules is importand\n",
    "    #Compinations of two or more words, are concatenated, in order to be considered as a single token\n",
    "    \n",
    "    #text = str(text)\n",
    "    text = p58.sub(' ',text)\n",
    "    \n",
    "    text = p5.sub(' λογαριασμος ',text)\n",
    "    text = p4.sub(' τηλεφωνο ',text)\n",
    "    text = p6.sub(' δενθελειδενενδιαφερεται ',text)\n",
    "    text = p10.sub(' δενθελειδενενδιαφερεται ',text)\n",
    "    text = p7.sub(' δενεχειδενμπορει ',text)\n",
    "    text = p8.sub(' δενειναιδιαθεσιμος ',text)\n",
    "    text = p9.sub(' ανεφικτη ',text)\n",
    "    text = text.replace('-banking','banking')\n",
    "    text = text.replace('v banking','vbanking')\n",
    "    text = text.replace('e banking','ebanking')\n",
    "    text = text.replace('follow up','followup')\n",
    "    text = text.replace('fup','followup')\n",
    "    text = text.replace('f/up','followup')\n",
    "    text = text.replace('πυρ/ριο','πυρασφαλιστηριο')\n",
    "    text = text.replace('safe drive','safedrive')\n",
    "    text = text.replace('safe pocket','safepocket')\n",
    "    text = text.replace('alphabank','alpha')\n",
    "    text = text.replace('sweet home smart','sweethomesmart')\n",
    "    text = text.replace('sweet home','sweethome')\n",
    "    text = text.replace('eξασφαλιζω','εξασφαλιζω')\n",
    "    text = text.replace('credit card','creditcard')\n",
    "    text = text.replace('debit card','debitcard')\n",
    "    text = text.replace('life cycle','lifecycle')\n",
    "    text = text.replace('π/κ','πκ')\n",
    "    text = text.replace('td','πκ')\n",
    "    text = text.replace('α/κ','ακ')\n",
    "    text = text.replace('δ/α','δεναπαντα ')\n",
    "    text = text.replace('εκτος αττικης','εκτοςαττικης ')\n",
    "    text = text.replace('paf payments','συναλλαγηpaf')\n",
    "    text = text.replace('paf online payments','συναλλαγηpaf')\n",
    "    text = text.replace('κτθ aps', 'καταθεσηaps')\n",
    "    text = text.replace('καταθεση μετρ. απο aps', 'καταθεσηaps')\n",
    "    text = text.replace('μτφ μεσω web','συστημικόμτφebanking')\n",
    "    text = text.replace('visa gold', 'visagold')\n",
    "    text = text.replace('αριθμος κυκλοφοριας', 'αριθμοςκυκλοφοριας')\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    #τδ\n",
    "    text = p1.sub(' δεναπαντα ',text)\n",
    "    text = p2.sub(' δεναπαντα ',text)\n",
    "    text = p3.sub(' δεντονβρηκα ',text)\n",
    "    text = p37.sub(' αριθμοςφορολογικουμητρωου ',text)\n",
    "    text = p36.sub(' αριθμοςφορολογικουμητρωου ',text)\n",
    "    text = p35.sub(' τιμολογια ',text)\n",
    "    text = p80.sub(' τιμολογια ',text)\n",
    "    text = p46.sub(' τιμολογια ',text)\n",
    "    text = p72.sub(' τιμολογια ',text)\n",
    "    text = p17.sub(' τιμολογια ',text)\n",
    "    text = p73.sub(' μηνες ',text)\n",
    "    text = p74.sub(' μηνες ',text)\n",
    "    text = p11.sub(' αριθμοςκυκλοφοριας ', text)\n",
    "    text = p12.sub(' ενοικιο ',text)\n",
    "    text = p13.sub(' ενοικιο ',text)\n",
    "    text = p62.sub(' ενοικιο ',text)\n",
    "    text = p14.sub(' γιατροι ',text)\n",
    "    text = p15.sub(' εξοφληση ',text)\n",
    "   \n",
    "    \n",
    "    text = p18.sub(' ΝΕΡΟ ',text)\n",
    "    text = p38.sub(' ΡΕΥΜΑ ',text)\n",
    "    text = p19.sub(' μισθοδοσιες ', text)\n",
    "    text = p65.sub(' επιδομα ',text)\n",
    "    text = p20.sub(' ακινητα ', text)\n",
    "    text = p41.sub(' ΕΝΦΙΑ ', text)\n",
    "    text = p21.sub(' φοροςεισοδηματος ', text)\n",
    "    text = p42.sub(' ΕΦΚΑ ', text)\n",
    "    text = p43.sub(' ΦΠΑ ', text)\n",
    "    text = p44.sub(' ΙΚΑ ', text)\n",
    "    text = p45.sub(' ΤΕΒΕ ', text)\n",
    "    text = p22.sub(' κοινοχρηστα ', text)\n",
    "    text = p23.sub(' συναλλαγεςγαμου ', text)\n",
    "    text = p78.sub(' συναλλαγεςγαμου ', text)\n",
    "    text = p24.sub(' συναλλαγεςγεννησηςβαφτισης ', text)\n",
    "    text = p79.sub(' συναλλαγεςγεννησηςβαφτισης ', text)\n",
    "    text = p25.sub(' στεγαστικα ', text)\n",
    "    text = p26.sub(' μεταφορες ', text)\n",
    "    text = p27.sub(' παραγγελιες ', text)\n",
    "    text = p28.sub(' οφειλες ', text)\n",
    "    text = p29.sub(' καταθεσεις ', text)\n",
    "    text = p30.sub(' προκαταβολες ', text)\n",
    "    text = p31.sub(' διατροφη ', text)\n",
    "    text = p32.sub(' carloan ', text)\n",
    "    text = p33.sub(' δανειο ', text)\n",
    "    text = p34.sub(' πιστωτικη ', text)\n",
    "    text = p48.sub(' δοσεις ',text)\n",
    "    text = p39.sub(' εξερχομενοεμβασμα ',text)\n",
    "    text = p40.sub(' εισερχομενοεμβασμα ',text)\n",
    "    text = p47.sub(' δωρεα ',text)\n",
    "    text = p49.sub(' τραπεζεςανταγωνισμου ',text)\n",
    "    text = p70.sub(' τραπεζεςανταγωνισμου ',text)\n",
    "    text = p50.sub(' ιδιωτικάσχολεια ',text)\n",
    "    text = p51.sub(' ΟΑΕΔ ',text)\n",
    "    text = p52.sub(' τεληκυκλοφοριας ',text)\n",
    "    text = p53.sub(' επιταγες ',text)\n",
    "    text = p54.sub(' μερισματα ',text)\n",
    "    text = p55.sub(' κολλεγια ',text)\n",
    "    text = p56.sub(' τηλεπικοινωνιες ',text)\n",
    "    text = p57.sub(' διδακτρα ',text)\n",
    "    text = p59.sub(' ΝΟΜΟΣ3869 ',text)\n",
    "    text = p60.sub(' carins ',text)\n",
    "    text = p77.sub(' carpurchase ',text)\n",
    "    text = p61.sub(' houseins ',text)\n",
    "    text = p76.sub(' αυτοκινητο ',text)\n",
    "    text = p16.sub(' ασφαλεια ',text)\n",
    "    text = p63.sub(' πληρωμες ',text)\n",
    "    text = p64.sub(' συμβολαια ',text) \n",
    "    text = p66.sub(' κρατηση ',text)\n",
    "    text = p67.sub(' διακοπες ',text)\n",
    "    text = p68.sub(' πετρελαιο ',text)\n",
    "    text = p69.sub(' καυσιμα ',text)\n",
    "    text = p71.sub(' αγορες ',text)\n",
    "    text = p75.sub(' φροντιστηρια ',text)\n",
    "\n",
    "    text = text.replace('daneio', 'δανειο')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "static-crossing",
   "metadata": {
    "gather": {
     "logged": 1642687225118
    }
   },
   "outputs": [],
   "source": [
    "def remove_ton(text):\n",
    "    #removes punctuation, αφαιρεί τους τόνους\n",
    "    diction = {'ά':'α','έ':'ε','ή':'η','ί':'ι','ό':'ο','ώ':'ω','ύ':'υ'}\n",
    "    for key in diction.keys():\n",
    "        text = text.replace(key, diction[key])\n",
    "    return text   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "painful-dairy",
   "metadata": {
    "gather": {
     "logged": 1642687225256
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def clean_text(text,sw):\n",
    "    #This function performs text cleansing and returns the clean and lemmatized version of the original text\n",
    "    #conver to lower text \n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # remove puncutation\n",
    "    #create a list from each text\n",
    "    text = [word.strip(string.punctuation) for word in text.split(\" \")]\n",
    "\n",
    "    # αφαιρούνται οι τόνοι\n",
    "    text = [remove_ton(x) for x in text]\n",
    "    \n",
    "    # remove stop words\n",
    "    text = [x for x in text if x not in sw]\n",
    "    \n",
    "    #remove quotes\n",
    "    text = [x.replace('quot;','').replace('&quot','') for x in text if x not in {'quot','amp'}]\n",
    "    \n",
    "    # remove words that contain numbers\n",
    "    #text = [word for word in text if not any(c.isdigit() for c in word)] #addition to return even empty tokens\n",
    "    \n",
    "    # remove empty tokens\n",
    "    text = [t for t in text if len(t) > 2] #addition to return even empty tokens\n",
    "    \n",
    "    # remove amp & quot\n",
    "    text = [x for x in text if x not in ['quot','amp']]\n",
    "    \n",
    "    # remove words with only one letter\n",
    "    #recreate text from list\n",
    "    text = \" \".join([t for t in text if len(t) > 2]) #addition to return even empty tokens\n",
    "    \n",
    "     #replacements either by rules or regular expressions\n",
    "    text = replaceTerm(text)\n",
    "    \n",
    "    # lemmatize text\n",
    "    text = \" \".join([t.lemma_ for t in nlp(text, disable=['tagger', 'parser', 'ner','tok2vec', 'morphologizer', 'parser', 'senter', 'attribute_ruler',  'ner'])])\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "spoken-novel",
   "metadata": {
    "gather": {
     "logged": 1642687225505
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def load_correctDict(ws):\n",
    "    #it creates a dictionary out of a dataset that containes pairs of (original term, corrected term)    \n",
    "    dataset = Dataset.get_by_name(ws, name='correct_Tokens')    \n",
    "    corDict = dict(dataset.to_pandas_dataframe().to_dict(\"split\")['data'])\n",
    "    return corDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "passing-suite",
   "metadata": {
    "gather": {
     "logged": 1642687225655
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def correct(x,corDict):\n",
    "    #uses the dictionary to correct the terms\n",
    "    if x in corDict.keys():\n",
    "        y = corDict[x]\n",
    "    else:\n",
    "        y = x\n",
    "    return y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "global-chess",
   "metadata": {
    "gather": {
     "logged": 1642687225796
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def get_ngrams(idf,mindf,minngram,maxngram):\n",
    "    #this function returns the bi-grams and tri-grams\n",
    "    tfidf = TfidfVectorizer(min_df = mindf,ngram_range = (minngram,maxngram))\n",
    "    tfidf_result = tfidf.fit_transform(idf['tokenized']).toarray()\n",
    "    tfidf_df = pd.DataFrame(tfidf_result, columns = tfidf.get_feature_names())\n",
    "    tfidf_df.columns = [str(x) for x in tfidf_df.columns]\n",
    "    df_i = pd.concat([df[['CON_ROW_ID']],tfidf_df],axis=1).melt(id_vars=['CON_ROW_ID'],value_vars = tfidf_df.columns).dropna()\n",
    "    df_i = df_i[df_i['value']>0]\n",
    "    return df_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "environmental-pointer",
   "metadata": {
    "gather": {
     "logged": 1642687225939
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def cleanComments(df,sw):\n",
    "    #applies the clean text function to all texts contained in the dataset\n",
    "    df = df[['CON_ROW_ID','CON_COMMENTS']]\n",
    "    df['tokenized'] = df['CON_COMMENTS'].apply(clean_text,args = (sw))\n",
    "    df = df.fillna('N/A')\n",
    "    df['variable'] = df['tokenized'].str.split()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "improving-smoke",
   "metadata": {
    "gather": {
     "logged": 1642687226079
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def getTokens(df,sw):\n",
    "    #The variable columns is a list. The explode method \"unpivots this list\"\n",
    "    df = cleanComments(df,sw)\n",
    "    df_f = df.explode('variable')[['CON_ROW_ID','variable']]\n",
    "    return df_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "personal-hands",
   "metadata": {
    "gather": {
     "logged": 1642687226360
    }
   },
   "outputs": [],
   "source": [
    "def getTokencount(df_f,minCount):\n",
    "    #calculate the number of occurances (counts) of each token\n",
    "    #tokens with count less than mincount are set to blank\n",
    "    tokenCount = df_f['variable'].value_counts().to_dict()\n",
    "    \n",
    "    df_f['value'] = df_f['variable'].map(tokenCount)\n",
    "   \n",
    "    df_f.loc[(df_f['value']<minCount), 'variable'] = ' ' #addition to return even empty tokens\n",
    "    \n",
    "    return df_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bizarre-finger",
   "metadata": {
    "gather": {
     "logged": 1642687226501
    }
   },
   "outputs": [],
   "source": [
    "def performNLP(workspace,minCount,mindf,minngram,maxngram,deleteEmptyTokens,df):\n",
    "    sw = loadStopWords(workspace)\n",
    "    \n",
    "    df = cleanComments(df,sw)\n",
    "    \n",
    "    df_f = getTokens(df,sw)\n",
    "\n",
    "    df_f.count()\n",
    "    \n",
    "    df_f = df_f.fillna(' ')\n",
    "    \n",
    "    df_f = getTokencount(df_f,minCount)\n",
    "    \n",
    "    #try:\n",
    "    #    df_f = df_f.append(get_ngrams(df,mindf,minngram,maxngram ))\n",
    "    #except:\n",
    "    #    print('no bigramms or trigramms were added')\n",
    "    \n",
    "    #corDict = load_correctDict(workspace)     \n",
    "    \n",
    "    #df_f['token'] = df_f['variable'].apply(lambda x : correct(x,corDict))\n",
    "    \n",
    "    #df_f.loc[(df_f['variable'].str.len() <2), 'variable'] = ' ' #single character tokens are set to blank\n",
    "    \n",
    "    df_f['token'] = df_f['variable']\n",
    "    \n",
    "    df_f['length'] = df_f['token'].str.len() #tokens length\n",
    "    \n",
    "    df_f.loc[(df_f['length']<3), ['token']] = ' ' #till two-character tokens are set to blank\n",
    "    \n",
    "    df_f = df_f.sort_values(['CON_ROW_ID','token'])\n",
    "    \n",
    "    if deleteEmptyTokens:\n",
    "        df_f = df_f[df_f['token'] != ' ']\n",
    "    \n",
    "    df_f = df_f[['CON_ROW_ID','token']].drop_duplicates()\n",
    "    \n",
    "    return df_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "synthetic-density",
   "metadata": {
    "gather": {
     "logged": 1642687226641
    }
   },
   "outputs": [],
   "source": [
    "def loadTexts(workspace,datasetName):\n",
    "    #loads the texts to be analyzed\n",
    "    dataset = Dataset.get_by_name(workspace, name=datasetName)\n",
    "    df = dataset.to_pandas_dataframe()\n",
    "    df= df[['CON_ROW_ID','CON_COMMENTS']]\n",
    "    return df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "intimate-architect",
   "metadata": {
    "gather": {
     "logged": 1642687435412
    }
   },
   "outputs": [],
   "source": [
    "def exportResults(workspace,datastore,fileName,df_f):\n",
    "    df_f.to_csv(fileName+'.txt',sep ='|',line_terminator='\\r\\n',index = False, encoding = 'UTF-8')\n",
    "    fil = [os.getcwd()+'/'+ fileName+'.txt']\n",
    "    #datastore.upload_files(fil, target_path='UI/NLP', overwrite=True, show_progress=True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-channels",
   "metadata": {},
   "source": [
    "<b>The commended-out code is for debuging purposes</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "choice-order",
   "metadata": {
    "gather": {
     "logged": 1642687227003
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#txt = 'H eurobank είναι καλύτερη τράπεζα στον κόσμο'\n",
    "#com = {'CON_ROW_ID':[1],'CON_COMMENTS':[txt]}\n",
    "#df = pd.DataFrame(com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "nasty-farmer",
   "metadata": {
    "gather": {
     "logged": 1642687235528
    }
   },
   "outputs": [],
   "source": [
    "df = loadTexts(workspace,datasetName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f83ed741-f1e0-461b-a11b-c4f5fb26ee16",
   "metadata": {
    "gather": {
     "logged": 1642687235674
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#df = df.head(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0cd6dc54-39db-47b6-9320-35cdea7b10cc",
   "metadata": {
    "gather": {
     "logged": 1642687235975
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CON_ROW_ID</th>\n",
       "      <th>CON_COMMENTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24398375</td>\n",
       "      <td>Olabode Kafayat               &amp;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24398376</td>\n",
       "      <td>ΤΠ292026 ΧΑΤΖΟΠΟΥΛΟΥ ΚΩΝΝΑ    &amp;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24398377</td>\n",
       "      <td>ΗΛΙΑΣ ΓΙΑΝΝΟΠΟΥΛΟΣ ΕΞΟΦΛΗΣΗ   &amp;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24398378</td>\n",
       "      <td>405202782 STOIXIMANGR mauromat&amp;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24398379</td>\n",
       "      <td>ΣΥΡΑΝΙΔΗΣ ΣΕ ΣΥΡΑΝΙΔΗΣ ΚΩΝΣΤΑΝ&amp;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>24398944</td>\n",
       "      <td>ΒΟΛΟΥΔΑΚΗΣ ΠΡΟΝΗΠΕΙΟ          &amp;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>24398945</td>\n",
       "      <td>405349002 STOIXIMANGR antonyk6&amp;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>24398946</td>\n",
       "      <td>405354282 STOIXIMANGR Kidib   &amp;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>24398947</td>\n",
       "      <td>405352132 STOIXIMANGR doctor12&amp;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>24398948</td>\n",
       "      <td>405357342 STOIXIMANGR afitilis&amp;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     CON_ROW_ID                     CON_COMMENTS\n",
       "0      24398375  Olabode Kafayat               &\n",
       "1      24398376  ΤΠ292026 ΧΑΤΖΟΠΟΥΛΟΥ ΚΩΝΝΑ    &\n",
       "2      24398377  ΗΛΙΑΣ ΓΙΑΝΝΟΠΟΥΛΟΣ ΕΞΟΦΛΗΣΗ   &\n",
       "3      24398378  405202782 STOIXIMANGR mauromat&\n",
       "4      24398379  ΣΥΡΑΝΙΔΗΣ ΣΕ ΣΥΡΑΝΙΔΗΣ ΚΩΝΣΤΑΝ&\n",
       "..          ...                              ...\n",
       "995    24398944  ΒΟΛΟΥΔΑΚΗΣ ΠΡΟΝΗΠΕΙΟ          &\n",
       "996    24398945  405349002 STOIXIMANGR antonyk6&\n",
       "997    24398946  405354282 STOIXIMANGR Kidib   &\n",
       "998    24398947  405352132 STOIXIMANGR doctor12&\n",
       "999    24398948  405357342 STOIXIMANGR afitilis&\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-graphic",
   "metadata": {
    "gather": {
     "logged": 1642687237665
    }
   },
   "outputs": [],
   "source": [
    "df_f = performNLP(workspace, minCount, mindf, minngram, maxngram, deleteEmptyTokens, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-lloyd",
   "metadata": {
    "gather": {
     "logged": 1642687237819
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#df_f.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-tulsa",
   "metadata": {
    "gather": {
     "logged": 1642687440674
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "exportResults(workspace,datastore,fileName,df_f)\n",
    "\n",
    "#run.complete()UT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-subdivision",
   "metadata": {
    "gather": {
     "logged": 1642687238391
    }
   },
   "outputs": [],
   "source": [
    "#pd.merge(df_f,df, how=\"inner\",on=['CON_ROW_ID']).head(1000000).to_excel('NLP{0}.xlsx'.format(fileName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-diameter",
   "metadata": {
    "gather": {
     "logged": 1642687238527
    }
   },
   "outputs": [],
   "source": [
    "#df_f['token'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-athens",
   "metadata": {
    "gather": {
     "logged": 1642687238662
    }
   },
   "outputs": [],
   "source": [
    "#df[df['CON_ROW_ID']==6530134]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistent-latex",
   "metadata": {
    "gather": {
     "logged": 1642687238798
    }
   },
   "outputs": [],
   "source": [
    "#df_f[df_f['token']=='αριθμοςκυκλοφοριας']\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "percent"
   }
  },
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
